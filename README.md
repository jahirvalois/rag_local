# Local RAG + Chat Assistant (Cross-Platform)

> Build and run **Retrieval-Augmented Generation (RAG)** + **Chat** assistant locally on **Windows** or **Linux**, using open-weight models.

---

## Overview

This project lets you:
- üóÇÔ∏è Ingest PDFs, TXTs, or Markdown files.
- üí¨ Chat naturally with your own documents.
- üß© Run everything locally ‚Äî even on CPU.
- ‚öôÔ∏è Upgrade to GPU or multi-GPU servers easily.

---

## Tech Stack

| Component | Tool | Purpose |
|------------|------|----------|
| **Model Runner** | [Ollama](https://ollama.ai) / [vLLM](https://github.com/vllm-project/vllm) | Local inference (CPU/GPU) |
| **Framework** | [LlamaIndex](https://github.com/run-llama/llama_index) | RAG orchestration and chat engine |
| **Vector DB** | [Chroma](https://www.trychroma.com) | Vector search and similarity |
| **Embeddings** | [Sentence-Transformers](https://www.sbert.net) | Text & image embeddings |
| **UI** | [Streamlit](https://streamlit.io) | Web-based Chat interface |

---

## Windows Setup

```powershell
# 1Ô∏è‚É£ Install dependencies
winget install Python.Python.3.11
winget install Ollama.Ollama

# 2Ô∏è‚É£ Pull starter models
ollama pull phi3:mini
ollama pull nomic-embed-text

# 3Ô∏è‚É£ Clone and setup project
git clone https://github.com/<your-repo>/rag_local.git
cd rag_local
python -m venv .venv
. .\.venv\Scripts\Activate.ps1
pip install -U -r requirements.txt

# 4Ô∏è‚É£ Build index and start chat
python .\app\build_index.py
streamlit run .\app\chat_ui.py
```

Then open **http://localhost:8501**

---

## Linux Setup

```bash
# 1Ô∏è‚É£ System setup
sudo apt update && sudo apt install -y python3 python3-venv python3-pip git curl

# 2Ô∏è‚É£ Install Ollama (model runner)
curl -fsSL https://ollama.com/install.sh | sh
ollama serve &

# 3Ô∏è‚É£ Pull models
ollama pull phi3:mini
ollama pull nomic-embed-text

# 4Ô∏è‚É£ Clone and configure
git clone https://github.com/<your-repo>/rag_local.git
cd rag_local
python3 -m venv .venv && source .venv/bin/activate
pip install -U -r requirements.txt

# 5Ô∏è‚É£ Index and run chat
python app/build_index.py
streamlit run app/chat_ui.py
```

---

## Environment Configuration (`app/.env`)

```env
LLM_MODEL=phi3:mini
EMBED_MODEL=BAAI/bge-small-en-v1.5
CHROMA_PATH=./chroma_db
DATA_DIR=./data
PERSIST_DIR=./storage
TOP_K=5
```

---

## Project Structure

```
rag-chat-local/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ build_index.py      # Builds document embeddings
‚îÇ   ‚îú‚îÄ‚îÄ chat_ui.py          # Streamlit multi-turn chat UI
‚îÇ   ‚îî‚îÄ‚îÄ .env                # Model and config
‚îú‚îÄ‚îÄ data/                   # PDFs, TXT, or MD files
‚îú‚îÄ‚îÄ chroma_db/              # Persistent vector DB
‚îú‚îÄ‚îÄ storage/                # LlamaIndex storage context
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üîÑ Updating Existing Builds

You can easily swap models in your `.env` to upgrade quality or speed.

### Example ‚Äì Chat model update
```env
# Old
LLM_MODEL=phi3:mini
# New
LLM_MODEL=llama3.1:8b
```

```bash
ollama pull llama3.1:8b
python app/build_index.py
streamlit run app/chat_ui.py
```

### Example ‚Äì RAG embedding model update
```env
# Old
EMBED_MODEL=BAAI/bge-small-en-v1.5
# New
EMBED_MODEL=BAAI/bge-large-en-v1.5
```

```bash
pip install -U sentence-transformers
python app/build_index.py
```

---

> üí° Tip: Start with `phi3:mini` for rapid CPU experimentation.  
> Upgrade to `llama3.1:8b` or `qwen2.5:7b` once your base workflow is stable.
